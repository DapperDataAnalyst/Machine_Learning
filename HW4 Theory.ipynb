{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $Pr(X=i)$ to be a valid probability mass function, it must be the case that $\\sum Pr(x) = 1$\n",
    "\n",
    "$$Pr(X=1) + Pr(X=2) + Pr(X=3) = 1$$\n",
    "$$\\theta_1 + 2\\theta_1 + \\theta_2 = 1$$\n",
    "$$3\\theta_1 + \\theta_2 = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given information we have that:\n",
    "$$Pr(D | \\theta) = (\\theta_1^{s_1} * (2\\theta_1)^{s_2} * \\theta_2^{s_3})$$\n",
    "\n",
    "Next we find the log probability as:\n",
    "$$log Pr(D | \\theta) = log(\\theta_1^{s_1} * (2\\theta_1)^{s_2} * \\theta_2^{s_3})$$\n",
    "$$= log\\theta_1^{s_1} + log(2\\theta_2)^{s_2} + log\\theta_2^{s_3}$$\n",
    "$$= s_1log\\theta_1 + s_2(log2 + log\\theta_1) + s_3log\\theta_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first find $\\hat{\\theta}$ by taking the derivative of the log likelihood with respect to $\\theta_1$. We also utilize the result found in Part (a) that $\\theta_2 = 1-3\\theta_1$:\n",
    "$$\\frac{d}{d\\theta_1}(log Pr(D | \\theta)) = \\frac{s_1}{\\theta_1} + \\frac{s_2}{\\theta_1} - \\frac{3s_3}{1-3\\theta_1} = 0$$\n",
    "$$\\frac{s_1+s_2}{\\theta_1} - \\frac{3s_3}{1-3\\theta_1} = 0$$\n",
    "$$\\frac{1-3\\theta_1}{\\theta_1} = \\frac{3s_3}{s_1 + s_2}$$\n",
    "$$\\frac{1}{\\theta_1} - 3 = \\frac{3s_3}{s_1 + s_2}$$\n",
    "$$\\frac{1}{\\theta_1} = \\frac{3s_3}{s_1 + s_2} + 3$$\n",
    "$$\\hat{\\theta_1} = \\frac{1}{\\frac{3s_3}{s_1 + s_2} + 3}$$\n",
    "$$\\hat{\\theta_1} = \\frac{s_1 + s_2}{3(s_1 + s_2 + s_3)}$$\n",
    "\n",
    "Substituting this $\\hat{\\theta_1}$, we can find $\\hat{\\theta_2}$ as:\n",
    "$$\\hat{\\theta_2} = 1-3\\hat{\\theta_1}$$\n",
    "$$\\hat{\\theta_2} = 1-3\\frac{s_1 + s_2}{3(s_1 + s_2 + s_3)}$$\n",
    "$$\\hat{\\theta_2} = \\frac{s_3}{s_1 + s_2 + s_3}$$\n",
    "\n",
    "Thus the maximum likelihood estimation for $\\hat{\\theta}$ is:\n",
    "$$\\hat{\\theta} = [\\frac{s_1 + s_2}{3(s_1 + s_2 + s_3)}, \\frac{s_3}{s_1 + s_2 + s_3}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information given in the question prompt we have:\n",
    "$$\\Pi_{i=1}^n Pr(x_1,\\dots,x_n | \\beta) = \\frac{1}{\\beta}e^{-\\frac{x_i}{\\beta}}$$\n",
    "\n",
    "Taking the log we have:\n",
    "$$log\\Pi_{i=1}^n Pr(x_1,\\dots,x_n | \\beta) = \\sum_{i=1}^n(log\\frac{1}{\\beta} - \\frac{x_i}{\\beta})$$\n",
    "$$= -nlog\\beta - \\frac{1}{\\beta}\\sum x_i$$\n",
    "\n",
    "Taking the derivative and setting equal to zero we have:\n",
    "$$\\frac{d}{d\\beta}log\\Pi_{i=1}^n Pr(x_1,\\dots,x_n | \\beta) = -\\frac{n}{\\beta} + \\frac{1}{\\beta^2}\\sum x_i = 0$$\n",
    "$$\\frac{1}{\\beta^2}\\sum x_i = \\frac{n}{\\beta}$$\n",
    "$$\\hat{\\beta} = \\frac{\\sum x_i}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find that the posterior is proportional to the likelihood times the prior:\n",
    "$$f(\\theta|data) \\propto f(data|\\theta) * f(\\theta)$$\n",
    "\n",
    "Note that the probability density function of a uniform distribution is typically represented as $\\frac{1}{b-a}$ and is thus a constant. Because of this, the probability desity function of the prior, $f(\\theta)$, can be absorbed into the proportion and omitted from our notation.\n",
    "\n",
    "The likelihood function takes the form of a binomial distribution as follows:\n",
    "$$f(data|\\theta) = {30\\choose5}\\theta^5 (1-\\theta)^{30-5}$$\n",
    "\n",
    "Finally we can write the posterior distribution as:\n",
    "$$f(\\theta|data) \\propto \\theta^5 (1-\\theta)^{30-5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given that $P(D|\\mu) \\sim N(\\mu,1) = \\Pi_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{(x_i - \\mu)^2}{2})$ and $P(\\mu) \\sim N(0,1) = \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{\\mu^2}{2})$\n",
    "\n",
    "Combining these distributions, we get the Bayesian posterior:\n",
    "$$P(\\mu|D) \\propto P(D|\\mu)*P(\\mu) = (\\Pi_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{(x_i - \\mu)^2}{2}))(\\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{\\mu^2}{2}))$$\n",
    "$$\\propto exp(-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2} - \\frac{\\mu^2}{2})$$\n",
    "$$= exp(-\\frac{1}{2}(A\\mu^2 - 2B\\mu + C))$$\n",
    "\n",
    "In this expression:\n",
    "$$A = \\sum_{i=1}^n \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_0^2}$$\n",
    "$$= \\frac{n}{1} + \\frac{1}{1} = n + 1$$\n",
    "\n",
    "And:\n",
    "$$B = \\sum_{i=1}^n \\frac{x_i}{\\sigma_1^2} + \\frac{\\mu_0}{\\sigma_0^2}$$\n",
    "$$= \\sum_{i=1}^n \\frac{x_i}{1} + \\frac{0}{1}$$\n",
    "$$= \\sum_{i=1}^n x_i$$\n",
    "\n",
    "From the lecture we found that by phrasing a Gaussian in terms of A, B, and C, we have the convenient result that the mean of the distribution is found as $\\frac{B}{A}$.\n",
    "\n",
    "Substituting in our A and B as found above:\n",
    "$$\\mu_{posterior} = \\frac{B}{A} = \\frac{\\sum_{i=1}^n x_i}{n + 1}$$\n",
    "\n",
    "Additionally, we found in the lecture that the variance of a Gaussian phrased in A, B, and C is $\\frac{1}{A}$.\n",
    "\n",
    "In our case:\n",
    "$$\\sigma_{posterior}^2 = \\frac{1}{A} = \\frac{1}{n + 1}$$\n",
    "\n",
    "We can now fully state the distribution of the posterior:\n",
    "$$p(\\mu|D) \\sim N(\\frac{\\sum_{i=1}^n x_i}{n + 1}, \\frac{1}{n + 1})$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
