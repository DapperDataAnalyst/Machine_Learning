{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case when f(x) = g(x), then\n",
    "$$P_{x \\sim D}[f(x) \\neq g(x)] = \\frac{1-E[(-1)(-1)]}{2} = \\frac{1-1}{2} = 0$$\n",
    "\n",
    "Similarly, in the case when f(x) $\\neq$ g(x), then\n",
    "$$P_{x \\sim D}[f(x) \\neq g(x)] = \\frac{1-E[(-1)(1)]}{2} = \\frac{1-(-1)}{2} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would still be true even if the domain were some other set, including the real set. The equality depends upon the outputs of f(x) and g(x) being boolean {-1, 1}, not upon the inputs being boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create a summation of indicator polynomials, with each polynomial term representing a leaf in the tree, as follows:\n",
    "$$P(x) = c_1 * I_1(x) + c_2 * I_2(x) + ... + c_t * I_t(x)$$\n",
    "\n",
    "Here each $I_i(x)$ is an indicator function that is 1 if the logical AND of all the conditions on the path from the root to that leaf is true, and 0 if the logical AND of all conditions from root to that leaf is false.\n",
    "\n",
    "Each $c_i$ is a coefficient of either -1 or +1, representing the boolean output of the decision tree indicated by the leaf node (i.e., the polynomial term in the summation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the positive column is 165.\n",
    "\n",
    "The sum of the negative column is 85.\n",
    "\n",
    "Thus, the total observations are 165 + 85 = 250.\n",
    "\n",
    "We first find $\\phi(Pr(Neg))$ as\n",
    "$$\\phi(Pr(Neg)) = 2(\\frac{85}{250})(1-\\frac{85}{250}) = 0.4488$$\n",
    "\n",
    "We now find the variable that has the greatest gain over this value. First for X:\n",
    "$$Pr(X=0)\\cdot\\phi(Pr(Neg|X=0) + Pr(X=1)\\cdot\\phi(Pr(Neg|X=1))$$\n",
    "$$= 0.5\\phi(\\frac{3}{10}) + 0.5\\phi(\\frac{2}{5})$$\n",
    "$$= 0.5(\\frac{21}{50}) + 0.5(\\frac{12}{50}) = 0.45$$\n",
    "Thus, $Gain_X = 0.4488 - 0.45 = -0.0012$\n",
    "\n",
    "Next for Y:\n",
    "$$Pr(Y=0)\\cdot\\phi(Pr(Neg|Y=0) + Pr(Y=1)\\cdot\\phi(Pr(Neg|Y=1))$$\n",
    "$$= 0.5\\phi(\\frac{5}{12}) + 0.5\\phi(\\frac{7}{26})$$\n",
    "$$= 0.5(\\frac{35}{72}) + 0.5(\\frac{133}{338}) = 0.439801118$$\n",
    "Thus, $Gain_Y = 0.4488 - 0.439801118 = 0.008998882$\n",
    "\n",
    "Lastly for Z:\n",
    "$$Pr(Z=0)\\cdot\\phi(Pr(Neg|Z=0) + Pr(Z=1)\\cdot\\phi(Pr(Neg|Z=1))$$\n",
    "$$= 0.5\\phi(\\frac{1}{2}) + 0.5\\phi(\\frac{5}{26})$$\n",
    "$$= 0.5(\\frac{1}{2}) + 0.5(\\frac{105}{338}) = 0.405325444$$\n",
    "Thus, $Gain_Z = 0.4488 - 0.405325444 = 0.043474556$\n",
    "\n",
    "\n",
    "Z has the greatest gain, so Z becomes the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chosen Z as the root, we test to determine if the left child of the root should be X or Y.\n",
    "\n",
    "$Pr(Neg|Z=0) = 0.5$\n",
    "$\\phi(Pr(Neg|Z=0)) = 0.5$\n",
    "\n",
    "First, for X|Z=0:\n",
    "$$Pr(X=0|Z=0)\\cdot\\phi(Pr(Neg|X=0,Z=0)) + Pr(X=1|Z=0)\\cdot\\phi(Pr(Neg|X=1,Z=0))$$\n",
    "$$= 0.5\\phi(\\frac{7}{16}) + 0.5\\phi(\\frac{5}{8})$$\n",
    "$$= 0.5(\\frac{63}{128}) + 0.5(\\frac{15}{32}) = 0.48046875$$\n",
    "\n",
    "Thus $Gain_{X|Z=0} = 0.5 - 0.48046875 = 0.01953125$\n",
    "\n",
    "Next for Y|Z=0:\n",
    "$$Pr(Y=0|Z=0)\\cdot\\phi(Pr(Neg|Y=0,Z=0)) + Pr(Y=1|Z=0)\\cdot\\phi(Pr(Neg|Y=1,Z=0))$$\n",
    "$$= 0.5\\phi(\\frac{7}{10}) + 0.5\\phi(\\frac{5}{14})$$\n",
    "$$= 0.5(\\frac{21}{50}) + 0.5(\\frac{45}{98}) = 0.439591837$$\n",
    "\n",
    "Thus $Gain_{X|Z=0} = 0.5 - 0.439591837 = 0.060408163$\n",
    "\n",
    "Y has the greatest gain given that Z=0, so the left child of the root is Y.\n",
    "By elimination, then, the right child of the root is X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now determine the leaves of the tree.\n",
    "\n",
    "When Z=0 and Y=0, there are 15 positive examples and 35 negative examples. Thus the left leaf of Y should be 0.\n",
    "\n",
    "When Z=0 and Y=1, there are 45 positive examples and 25 negative examples. Thus the right leaf of Y should be 1.\n",
    "\n",
    "When Z=1 and X=0, there are 60 positive examples and 10 negative examples. Thus the left leaf of X should be 1.\n",
    "\n",
    "When Z=1 and X=1, there are 45 positive examples and 15 negative examples. Thus the right leaf of X should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this classification scheme, our tree would incorrectly assign 65 examples.\n",
    "\n",
    "Thus the tree's overall accuracy is $\\frac{250-65}{250} = 0.74$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create this algorithm, we could first draw $m = O(\\frac{1}{\\epsilon}\\cdot log(\\frac{1}{\\delta}))$ examples (x,y) from the distribution.\n",
    "\n",
    "Then we could sort the examples by their x values. In the sorted examples, we could find points where the labels change.\n",
    "\n",
    "For each midpoint $\\theta$ we could predict the labels $h_\\theta(x)$ for all the examples and count the misclassified examples. We would then choose the threshold $\\theta$ that yields the lowest count of misclassified examples.\n",
    "\n",
    "Lastly we return the classifier $h_\\theta(x)$ that uses the ideal $\\theta$ that we found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the bound holds for all possible hypotheses, we divide the set of all hypotheses into two groups:\n",
    "\n",
    "1. Hypotheses that classify the block of examples correctly.\n",
    "2. Hypotheses that classify the block of examples incorrectly (i.e., err(h) > Îµ).\n",
    "\n",
    "Since each block of size k is guaranteed to be correctly classified by some hypothesis from group 1 (by construction), and the number of hypotheses in group 2 is at most t (since algorithm A has a mistake bound t), you need to ensure that your sample size m is large enough to cover both group 1 and group 2:\n",
    "$$m \\geq size \\space of \\space group \\space 1 + size \\space of \\space group \\space 2 \\leq k + t$$\n",
    "\n",
    "### Pseudocode for Algorithm B:\n",
    "1. Initialize variables $\\epsilon$ and $\\delta$.\n",
    "2. Calculate the required t (mistake bound) from your knowledge.\n",
    "3. Calculate the required k using your calculations from part 5a, ensuring $P[err(h') > \\epsilon] \\leq \\delta'$.\n",
    "4. Calculate the required m = k + t.\n",
    "5. For i = 1 to m:\n",
    "   a. Draw an example x from distribution D.\n",
    "   b. Use learner A to classify x.\n",
    "6. Calculate the empirical error of the hypothesis $h_B$ generated by B on the m examples.\n",
    "7. Use results from probability theory (e.g., Hoeffding's inequality) to show that with high probability (at least 1 - $\\delta$), the empirical error of $h_B$ is close to the true error of the target concept c.\n",
    "8. Return $h_B$ as the output hypothesis.\n",
    "\n",
    "### Proof of Finite Sample Complexity:\n",
    "We need to show that there exists a finite number of m examples for B to PAC-learn C for all values of $\\delta$ and $\\epsilon$. We'll lower bound m by a function of $\\epsilon$, $\\delta$, and t.\n",
    "\n",
    "From step 3 of algorithm B, we have determined the required k such that $P[err(h') > \\epsilon] \\leq \\delta'$. Now, we need to take into account the maximum number of hypotheses in group 2, which is bounded by t (the mistake bound of learner A). This ensures that algorithm B covers all hypotheses that A could output, as discussed in part 5b.\n",
    "\n",
    "So, we have:\n",
    "\n",
    "The number of hypotheses in group 1 is at most k.\n",
    "The number of hypotheses in group 2 is at most t.\n",
    "The total number of hypotheses that algorithm B must account for is k + t.\n",
    "\n",
    "Thus, we set m = k + t, which guarantees that algorithm B generates a sufficient number of examples to cover all possible hypotheses. This ensures that B can PAC-learn C with respect to distribution D for any values of $\\delta$ and $\\epsilon$.\n",
    "\n",
    "In conclusion, by setting m = k + t, where k is derived from part 5a and t is the mistake bound of learner A, algorithm B satisfies the PAC-learning requirements. This ensures that B can output a hypothesis with a true error at most $\\epsilon$ with probability at least 1 - $\\delta$, given the existence of an efficient mistake-bounded learner A.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
